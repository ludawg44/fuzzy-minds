# Chapter 4 Training Models

There's a lot of math concepts in this chapter, specifically in Linear Algebra. Review this [linear algebra](https://github.com/ageron/handson-ml2/blob/master/math_linear_algebra.ipynb) repo Aurelien Geron created on GitHub.

## Linear Regression

This quick section goes over a linear regression model. You haven't trained anything yet. 

Key vocabulary: 
- bias term (intercept term)
- parameter vector
- feature vector
- column vectors
- Root Means Square Error (RMSE)

### The Normal Equation

Review the Normal Equalation. Review "from sklearn.linear_model import LinearRegression"

Key vocabulary: 
- Normal Equation
- closed-form solution
- pseudoinverse
- Singular Value Decomposition (SVD)

### Computational Complexity

Regarding the Linear Regression model, the computational complexity is linear with regard to both the number of instances you want to make predictions on and the number of features. 

Key vocabulary: 
- computational complexity

## Gradient Descent

A lot of great images that explain the concepts. 

Key vocabulary: 
- gradient descent
- random initialization
- converges
- learning rate
- local minimum 
- global minimum
- convex function
- parameter space

### Batch Gardient Descent



Key vocabulary: 
- partial derivative 
- partial derivatives of the cost function
- gradient vector of the cost function
- batch gradient descent
- tolerance 

### Stochastic Gradient Descent

### Mini-batch Gradient Descent

## Polynomial Regression

## Learning Curves

## Regularized linear Models

### Ridge Regression

### Lasso Regression

### Elastic Net

### Early Stopping

## Logistic Regression

### Estimating Probabilities

### Training and Cost Function

### Decision Boundaries

### Softmax Regression

## Exercises 
