# Chapter 5 Support Vector Machines

- [Linear SVM Classification](#linear-svm-classification)
  - [Soft Margin Classification](#soft-margin-classification)
- [Nonlinear SVM Classification](#nonlinear-svm-classification)
  - [Polynomial Kernel](#polynomial-kernel)
  - [Similarity Features](#similarity-features)
  - [Gaussian RBF Kernel](#gaussian-rbf-kernel)
  - [Computational Complexity](#computational-complexity)
- [SVM Regression](#svm-regression)
- [Under the Hood](#under-the-hood)
  - [Decision Function and Predictions](#decision-function-and-predictions)
  - [Training Objective](#training-objective)
  - [Quadratic Programming](#quadratic-programming)
  - [The Dual Problem](#the-dual-problem)
  - [Kernelized SVMs](#kernelized-svms)
  - [Online SVMs](#online-svms)

## Linear SVM Classification

Key vocabulary: 
- support vector machine
- linearly separable
- large margin classification
- support vectors

### Soft Margin Classification

Key vocabulary:
- hard margin classification
- margin violations
- soft margin classification
- hinge loss

## Nonlinear SVM Classification

Python: SciKit-Learn's "Pipeline" & "PolynomialFeatures" from "sklearn.pipeline" & "sklearn.preprocessing" from chapter 4. 

### Polynomial Kernel

Key vocabulary:
- kernel trick: makes it possoible to get the same result as if you had added many polynomial features, even with very high-degree polynomials, without actually having to add them. 

### Similarity Features

Key vocabulary:
- similarity function
- landmark
- Radial Basis Function (RBF)

### Gaussian RBF Kernel

### Computational Complexity

## SVM Regression

## Under the Hood

### Decision Function and Predictions

### Training Objective

### Quadratic Programming

### The Dual Problem

### Kernelized SVMs

### Online SVMs

