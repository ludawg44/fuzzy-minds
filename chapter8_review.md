# Chapter 8 Dimensionality Reduction

- [The Curse of Dimensionality](#the-curse-of-dimensionality)
- [Main Approaches for Dimensionality Reduction](#main-approaches-for-dimensionality-reduction)
- [Projection](#projection)
- [Manifold Learning](#manifold-learning)
- [PCA](#PCA)
- [Preserving the Varience](#preserving-the-varience)
- [Principal Components](#principal-components)
- [Projecting Down to d Dimensions](#projecting-down-to-d-dimensional)
- [Using Scikit-Learn](#using-scikit-learn)
- [Explained Varience Ratio](#explained-varience-ratio)
- [Choosing the Right Number of Dimensions](#choosing-the-right-number-of-dimensions)

## The Curse of Dimensionality
- Point, segment, square, cube, and tesseract (0D to 4D hypercubes)
- The more dimensions the training set has, the greater the risk of overfitting it.

## Main Approaches for Dimensinoality Reduction
- 2 main approaches to reducgin dimensionality: projection & manifold learning

### Projection


### Manifold Learning

## PCA

## Preserving the Varience

## Principal Components

# Projecting Down to d Dimension

# Using Scikit-Learn

# Explained Variance Ratio

# Choosing the Right Number of Dimensions

# PCA for Compression

# Randomized PCA

# Incremental PCA

# Kernel PCA

# Selecting a Kernel and Tuning Hyperparameters

# LLE

# Other Dimensionality Reduction Techniques


___

#### TABLED ITEMS
- Pending

#### GROUP WORK
- Pending

#### STATISTICS
- Pending

#### PYTHON
- Pending

#### MATHS
- Pending

#### LINKS
- Pending

#### BLOGS
- Pending

#### DATASETS
- Pending
